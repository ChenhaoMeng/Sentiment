import pandas as pd
import json
import os
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import chi2

# =================é…ç½®åŒºåŸŸ=================
FILE_PATH_05B = 'Qwen2.5-0.5B_result_full_text.jsonl'
FILE_PATH_15B = 'Qwen2.5-1.5B_result_full_text.jsonl'
# ==========================================

plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'sans-serif'] 
plt.rcParams['axes.unicode_minus'] = False

def load_results_optimized(filepath):
    print(f"\nğŸ“– [Loading] å¤„ç†æ–‡ä»¶: {filepath}")
    if not os.path.exists(filepath):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {filepath}")
        return None
    
    try:
        df = pd.read_json(filepath, lines=True)
        if 'is_correct' in df.columns and 'correct' not in df.columns:
            df['correct'] = df['is_correct']
        if 'file_source' not in df.columns:
            df['file_source'] = os.path.basename(filepath)
        df['original_row_index'] = df.index
        return df
    except Exception as e:
        print(f"âŒ è¯»å–å‘ç”Ÿé”™è¯¯: {e}")
        return None

def calculate_mcnemar(matrix_df):
    try:
        b = matrix_df.loc[matrix_df['Category'].str.contains('Fixed'), 'Count'].values[0]
        c = matrix_df.loc[matrix_df['Category'].str.contains('Regression'), 'Count'].values[0]
    except IndexError:
        return 0.0, 1.0
    
    if b + c == 0: return 0.0, 1.0
    statistic = (b - c)**2 / (b + c)
    p_value = 1 - chi2.cdf(statistic, 1)
    return statistic, p_value

def analyze_comparison(df_small, df_large, name_s="0.5B", name_l="1.5B"):
    print("="*40)
    print("ğŸ“Š å¼€å§‹å¯¹æ¯”åˆ†æ")
    print("="*40)

    # 1. æ•°æ®å¯¹é½
    align_keys = ['file_source', 'phenomenon', 'UID']
    if 'pair_id' in df_small.columns and 'pair_id' in df_large.columns:
        align_keys.append('pair_id')
        print("âœ… ä½¿ç”¨ pair_id è¿›è¡Œç²¾ç¡®å¯¹é½")
    else:
        align_keys.append('original_row_index')
        print("âš ï¸ æœªå‘ç° pair_idï¼Œä½¿ç”¨åŸå§‹è¡Œå· (row_index) å¯¹é½")

    meta_cols = align_keys + ['sentence_good', 'sentence_bad'] 
    existing_meta = [c for c in meta_cols if c in df_small.columns]
    
    df_compare = pd.merge(
        df_small[existing_meta + ['correct']],
        df_large[existing_meta + ['correct']],
        on=existing_meta,
        suffixes=(f'_{name_s}', f'_{name_l}')
    )
    print(f"ğŸ”— æˆåŠŸå¯¹é½æ ·æœ¬æ•°: {len(df_compare)}")

    # 2. æ ¸å¿ƒè®¡ç®—
    col_s = f'correct_{name_s}'
    col_l = f'correct_{name_l}'
    
    df_compare['both_correct'] = df_compare[col_s] & df_compare[col_l]
    df_compare['both_wrong'] = (~df_compare[col_s]) & (~df_compare[col_l])
    df_compare['fixed'] = (~df_compare[col_s]) & df_compare[col_l]
    df_compare['regression'] = df_compare[col_s] & (~df_compare[col_l])

    # 3. æ€»ä½“ç»Ÿè®¡
    acc_s = df_compare[col_s].mean()
    acc_l = df_compare[col_l].mean()
    print(f"\nğŸ† [æ€»ä½“è¡¨ç°]")
    print(f"  â€¢ {name_s} Acc: {acc_s:.2%}")
    print(f"  â€¢ {name_l} Acc: {acc_l:.2%}")
    print(f"  â€¢ Delta:      {acc_l - acc_s:+.2%}")

    # 4. McNemar æ£€éªŒ
    matrix_data = [
        {'Category': 'Both Correct (Easy)', 'Count': df_compare['both_correct'].sum()},
        {'Category': 'Both Wrong (Hard)',   'Count': df_compare['both_wrong'].sum()},
        {'Category': f'{name_l} Fixed (Improved)',     'Count': df_compare['fixed'].sum()},
        {'Category': f'{name_s} Won (Regression)',     'Count': df_compare['regression'].sum()}
    ]
    df_matrix = pd.DataFrame(matrix_data)
    stat, p_val = calculate_mcnemar(df_matrix)
    print(f"\nğŸ§© [æ˜¾è‘—æ€§åˆ†æ] McNemar p-value: {p_val:.4e}")

    # 5. Phenomenon ç»Ÿè®¡
    phenom_group = df_compare.groupby('phenomenon')[[col_s, col_l]].mean()
    phenom_group['Delta'] = phenom_group[col_l] - phenom_group[col_s]
    phenom_group = phenom_group.sort_values('Delta', ascending=False)

    return df_compare, phenom_group, df_matrix

# ================= æ–°å¢ï¼šUID è¯¦ç»†åˆ†æå‡½æ•° =================
def analyze_uid_stats(df_compare, name_s="0.5B", name_l="1.5B"):
    """
    è®¡ç®—æ¯ä¸ª UID çš„è¯¦ç»†å‡†ç¡®ç‡å’Œ Delta
    """
    print(f"\nğŸ”¬ [UID ç²’åº¦è¯¦ç»†åˆ†æ]")
    col_s = f'correct_{name_s}'
    col_l = f'correct_{name_l}'

    # æŒ‰ Phenomenon å’Œ UID åˆ†ç»„èšåˆ
    uid_stats = df_compare.groupby(['phenomenon', 'UID']).agg(
        count=('file_source', 'count'),       # æ ·æœ¬æ•°é‡
        acc_small=(col_s, 'mean'),            # å°æ¨¡å‹å‡†ç¡®ç‡
        acc_large=(col_l, 'mean')             # å¤§æ¨¡å‹å‡†ç¡®ç‡
    ).reset_index()

    # è®¡ç®—å·®å€¼
    uid_stats['delta'] = uid_stats['acc_large'] - uid_stats['acc_small']

    # æŒ‰æå‡å¹…åº¦é™åºæ’åˆ—
    uid_stats = uid_stats.sort_values('delta', ascending=False)

    # æ‰“å° Top 5 æå‡
    print("\nğŸš€ æå‡æœ€å¤§çš„ Top 5 UID:")
    print(uid_stats.head(5).to_string(index=False, formatters={
        'acc_small': '{:.1%}'.format, 'acc_large': '{:.1%}'.format, 'delta': '{:+.1%}'.format
    }))

    # æ‰“å° Top 5 å€’é€€
    print("\nğŸ“‰ å€’é€€æœ€å¤§çš„ Top 5 UID (Regression):")
    print(uid_stats.tail(5).to_string(index=False, formatters={
        'acc_small': '{:.1%}'.format, 'acc_large': '{:.1%}'.format, 'delta': '{:+.1%}'.format
    }))

    return uid_stats

def visualize_results(df_phenom, df_matrix):
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Delta Plot
    colors = ['#ff7675' if x < 0 else '#55efc4' for x in df_phenom['Delta']]
    sns.barplot(x=df_phenom['Delta'], y=df_phenom.index, ax=axes[0], hue=df_phenom.index, palette=colors, legend=False)
    axes[0].set_title('Accuracy Improvement by Phenomenon')
    axes[0].axvline(0, color='black', linestyle='--')
    
    # Donut Chart
    axes[1].pie(df_matrix['Count'], labels=df_matrix['Category'], autopct='%1.1f%%', 
                startangle=140, colors=['#dfe6e9', '#2d3436', '#00b894', '#d63031'], 
                wedgeprops=dict(width=0.4))
    axes[1].set_title('Agreement Distribution')
    
    plt.tight_layout()
    plt.show()

# =================æ‰§è¡Œé€»è¾‘=================
if __name__ == "__main__":
    df_05 = load_results_optimized(FILE_PATH_05B)
    df_15 = load_results_optimized(FILE_PATH_15B)

    if df_05 is not None and df_15 is not None:
        # 1. åŸºç¡€å¯¹æ¯”
        df_merged, df_phenom_stats, df_consistency = analyze_comparison(df_05, df_15)
        
        # 2. [æ–°å¢] UID ç²’åº¦åˆ†æ
        df_uid_stats = analyze_uid_stats(df_merged)
        
        # 3. å¯è§†åŒ–
        visualize_results(df_phenom_stats, df_consistency)
        
        # 4. ä¿å­˜æ–‡ä»¶
        # ä¿å­˜åˆå¹¶åçš„åŸå§‹æ•°æ®
        df_merged.to_csv('analysis_comparison_result.csv', index=False, encoding='utf-8-sig')
        # ä¿å­˜ UID ç»Ÿè®¡è¡¨
        df_uid_stats.to_csv('analysis_uid_breakdown.csv', index=False, encoding='utf-8-sig')
        
        print("\nğŸ’¾ æ–‡ä»¶ä¿å­˜æˆåŠŸ:")
        print("  1. analysis_comparison_result.csv (æ‰€æœ‰æ ·æœ¬è¯¦æƒ…)")
        print("  2. analysis_uid_breakdown.csv     (å„ UID å‡†ç¡®ç‡ç»Ÿè®¡è¡¨)")